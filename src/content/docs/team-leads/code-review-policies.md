---
title: Code Review Policies for AI-Generated Code
description: Adapting review practices for the agentic era
sidebar:
  order: 2
---

AI-generated code changes the code review dynamic. Your policies need to adapt.

## The core question

Should AI-generated code be reviewed differently than human-written code?

**Arguments for treating it the same:**

- Code is code. Quality standards shouldn't depend on origin.
- Creates unnecessary bureaucracy.
- Difficult to enforceâ€”how do you know what's AI-generated?

**Arguments for different treatment:**

- AI makes predictable mistakes humans don't.
- Reviewers can focus attention where it matters.
- Creates accountability and tracking.

Most teams land somewhere in the middle.

## Policy options

### Option 1: No differentiation

AI-generated code goes through the same review process as all code. Simple and transparent.

**Works when:**

- Team is small and high-trust
- All engineers are skilled at validating AI output
- Code review is already rigorous

### Option 2: Disclosure only

Authors disclose when PRs contain significant AI-generated code. Review process stays the same.

**Works when:**

- You want transparency without bureaucracy
- Trust reviewers to adjust attention appropriately
- Data collection for internal analysis is useful

### Option 3: Tiered review

Different review depth based on code criticality and AI involvement.

- **Critical paths + AI:** Senior reviewer required, extra scrutiny
- **Low-risk + AI:** Standard review
- **No AI:** Standard review

**Works when:**

- Risk tolerance varies by codebase area
- Senior engineers can provide targeted oversight
- Process overhead is acceptable

### Option 4: AI-assisted review

Use AI tools to pre-review AI-generated code. Human reviewers focus on what AI can't catch.

**Works when:**

- Team is comfortable with AI review tools
- Clear understanding of what AI reviewers miss
- Volume of PRs makes assistance valuable

## Review checklist for AI code

Train your reviewers to watch for:

### AI-specific issues

- [ ] Hallucinated APIs or methods
- [ ] Plausible but incorrect logic
- [ ] Missing edge case handling
- [ ] Inconsistent with existing patterns
- [ ] Over-engineered for the task

### Standard issues (always applicable)

- [ ] Meets requirements
- [ ] Handles errors appropriately
- [ ] Security considerations addressed
- [ ] Test coverage adequate
- [ ] Documentation updated

## The reviewer skill gap

Some reviewers are better at catching AI mistakes than others. This is a trainable skill.

**Build it by:**

- Sharing examples of AI failures caught in review
- Pairing junior reviewers with experienced ones
- Creating a team knowledge base of AI pitfalls

## Disclosure guidelines

If you require disclosure, make it easy:

**Clear trigger:** "Disclose when more than ~20% of the PR was generated by AI tools."

**Simple mechanism:** A checkbox in PR template, or a tag/label.

**No stigma:** Disclosure shouldn't trigger bias. It's information, not judgment.

**Useful metadata:** Which tool? What prompts? This helps with learning.

## What not to do

**Don't create a separate "AI code" branch.** This creates integration nightmares and defeats the purpose.

**Don't require manager approval for AI usage.** This kills adoption through bureaucracy.

**Don't ignore the conversation.** Pretending AI isn't changing your workflow doesn't help anyone.

---

## Further reading

_Links to external resources coming soon:_

- Sample code review policies
- Review training materials
- Industry standards and emerging practices
