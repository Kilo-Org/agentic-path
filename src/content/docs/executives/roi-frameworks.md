---
title: ROI Frameworks and Realistic Expectations
description: Understanding what returns are actually achievable
sidebar:
  order: 2
---

The marketing says 10x. The reality is more nuanced. Here's how to think about returns—and set expectations that survive contact with reality.

## The productivity claim landscape

Vendors claim:

- "10x developer productivity"
- "Replace half your engineering team"
- "Ship in days what used to take months"

Research shows:

- 10-30% improvement in task completion time
- Higher satisfaction among developers
- Significant variation by task type and individual

The gap between claims and evidence is worth understanding.

## Where gains actually come from

### Direct productivity

**Time savings on specific tasks:**

- Boilerplate generation: 50-80% faster
- Test writing: 30-50% faster
- Documentation: 40-60% faster
- Bug fixes with clear repro: 20-40% faster

**Complex implementation:** Minimal measurable improvement, sometimes negative (time spent prompting > time saved).

### Indirect benefits

**Capability expansion:** Engineers tackle tasks they wouldn't have attempted.

**Onboarding acceleration:** New hires reach productivity faster with agent assistance.

**Knowledge democratization:** Junior developers access expertise embedded in AI training.

**Developer satisfaction:** People often prefer agent-assisted work (though not universally).

### What doesn't change

**Architecture decisions:** Still human.
**Complex debugging:** Often slower with agents.
**Domain expertise:** Can't be delegated.
**Integration work:** High-context, agent-hostile.

## Building an ROI model

### Costs

**Tool costs:**

- Per-seat licensing
- API usage (for custom solutions)
- Infrastructure for self-hosted options

**Adoption costs:**

- Training time
- Process change management
- Initial productivity dip

**Oversight costs:**

- Additional code review time
- Security review requirements
- Quality assurance expansion

### Benefits

**Time savings:**

- Engineer hours × improvement % × adoption rate
- Be conservative: assume 15-20% improvement on applicable tasks

**Quality improvements:**

- Reduced bugs? (Data unclear—may go either direction)
- Better documentation? (Usually yes)
- Higher test coverage? (Usually yes, if prioritized)

**Capability gains:**

- New projects possible with existing headcount
- Faster exploration and prototyping
- Competitive responsiveness

### The honest math

For a 50-person engineering team:

- Tool cost: ~$500-2000/seat/year = $25k-100k annually
- Adoption investment: 20-40 hours/person = $200-400k one-time equivalent
- Realistic productivity gain: 15% on 60% of work = 9% overall
- Value of 9% on $10M engineering spend = $900k

Payback period: 3-12 months depending on assumptions.

Not transformational. But clearly positive.

## Setting expectations

### What to tell the board

"We expect 10-30% productivity improvement on applicable tasks, translating to single-digit overall efficiency gains. More importantly, we're building capability for a future where these tools are essential."

### What to tell engineering

"Start using these tools where they help. Don't force it where they don't. We're investing in learning, not mandating universal adoption."

### What to tell yourself

"This is a capability investment, not a cost-cutting exercise. The real value emerges over 2-3 years as the technology improves and our skills compound."

## Warning signs in ROI projections

Be skeptical of projections that:

- Assume universal adoption
- Use vendor productivity claims without adjustment
- Ignore adoption costs
- Assume immediate full benefit without learning curve
- Don't account for tasks where agents don't help

---

## Further reading

_Links to external resources coming soon:_

- Industry benchmarks on AI ROI
- Academic studies on productivity impact
- CFO frameworks for AI investment
